{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tigeranalytics.udemy.com/course/databricks-refresher/learn/lecture/34074836?start=210#overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n - What is Databricks\\n - Databrikcs Component\\n   * Workspace\\n   * Users\\n   * Groups\\n   * Clusters\\n   * Installation of libraries in clusters\\n   * Notebooks\\n   * Attach notebooks to clusters\\n   * Pools\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Databricks #\n",
    "'''\n",
    " - What is Databricks\n",
    " - Databricks Component\n",
    "   * Workspace\n",
    "   * Users\n",
    "   * Groups\n",
    "   * Clusters\n",
    "   * Installation of libraries in clusters\n",
    "   * Notebooks\n",
    "   * Attach notebooks to clusters\n",
    "   * Pools\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n - URL is based on AWS, AZURE, GCP \\n\\n - Workspace\\n  * Data Sceient & Engineering\\n     * Notebook \\n     * Data Import \\n     * Partner Connect \\n  * Machine Learning\\n  * SQL\\n\\n\\n# Data Engineering \\n - Workspace\\n - Repos\\n - Data\\n - Compute\\n - Workflows\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is databricks\n",
    "# Databricks Components\n",
    "'''\n",
    " - URL is based on AWS, AZURE, GCP \n",
    "\n",
    " - Workspace\n",
    "  * Data Sceient & Engineering\n",
    "     * Notebook \n",
    "     * Data Import \n",
    "     * Partner Connect \n",
    "  * Machine Learning\n",
    "  * SQL\n",
    "\n",
    "\n",
    "# Data Engineering \n",
    " - Workspace\n",
    " - Repos\n",
    " - Data\n",
    " - Compute\n",
    " - Workflows\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks Compoenent  : Users\n",
    "# Databricks Compoenent  : Groups\n",
    "'''\n",
    " - Enttitlments\n",
    "   * Allow unrestricted cluster creation\n",
    "   * Databricks SQL Acess\n",
    "   * Workspace Acess\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n  * Two Types of Cluster\\n   - All-Purpose\\n        * Standard\\n        * Single Node Cluster\\n        * High Concurrent Cluster\\n\\n   - Job Clusters\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Databricks Cluster\n",
    "'''\n",
    "  * Two Types of Cluster\n",
    "   - All-Purpose\n",
    "        * Standard\n",
    "        * Single Node Cluster\n",
    "        * High Concurrent Cluster\n",
    "\n",
    "   - Job Clusters\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n  - Name: tigerCluster\\n  - ClusterMode: Standard\\n  - Databricks runtime version: Runtime 11.0\\n  - * Use Photon Acclreation\\n  - Autopilot options\\n    * Enable Autoscaling\\n    *  Terminate After : 120 minutes \\n\\n - Worker types: \\n     * min Workers \\n     * max Workers\\n\\n - Driver Types\\n - Advance Options\\n   * Instance \\n   * Spark \\n   * Tags \\n  - Logging\\n  - Init Scripts\\n  - SSH\\n\\n  - Create Cluster  \\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to create a cluster \n",
    "# Compute > Create Cluster \n",
    "'''\n",
    "  - Name: tigerCluster\n",
    "  - ClusterMode: Standard\n",
    "  - Databricks runtime version: Runtime 11.0\n",
    "  - * Use Photon Acclreation\n",
    "  - Autopilot options\n",
    "    * Enable Autoscaling\n",
    "    *  Terminate After : 120 minutes \n",
    "\n",
    " - Worker types: \n",
    "     * min Workers \n",
    "     * max Workers\n",
    "\n",
    " - Driver Types\n",
    " - Advance Options\n",
    "   * Instance \n",
    "   * Spark \n",
    "   * Tags \n",
    "  - Logging\n",
    "  - Init Scripts\n",
    "  - SSH\n",
    "\n",
    "  - Create Cluster  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Installtion of Liraries in Clusters\n",
    "'''\n",
    " - Libraries\n",
    "     * Install Libraries\n",
    "        - PyPu\n",
    "           - Package : pyspft\n",
    "           - Repostirory: \n",
    "           - Install\n",
    "\n",
    "    * Sleect Uninstalled \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n - worksapsce > NAME > Create > \\n      Name: Sample \\n      Default Langaue: Python , \\n      Cluster > Create\\n      \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notebooks\n",
    "'''\n",
    " - worksapsce > NAME > Create > \n",
    "      Name: Sample \n",
    "      Default Langaue: Python , \n",
    "      Cluster > Create\n",
    "      \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n - Schedule\\n - Job Name: \\n - Schedule: \\n - Cluster : \\n    * New Job Cluster: Edit Job Cluster , Confirm \\n - Parameters\\n - Alerts\\n - Create\\n\\n\\n # Check in Job Clusters \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jobs \n",
    "'''\n",
    "%sql \n",
    "\n",
    "select 2+2\n",
    "\n",
    "%scala \n",
    "\n",
    "'''\n",
    "# How to create Job \n",
    "'''\n",
    " - Schedule\n",
    " - Job Name: \n",
    " - Schedule: \n",
    " - Cluster : \n",
    "    * New Job Cluster: Edit Job Cluster , Confirm \n",
    " - Parameters\n",
    " - Alerts\n",
    " - Create\n",
    "\n",
    "\n",
    " # Check in Job Clusters \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pools \n",
    "'''\n",
    " - Create Pool \n",
    " - Name: demodatabrickspool\n",
    " - Min Idle: 0 \n",
    " - Max Capacity: OPtional \n",
    " - Instance Type\n",
    " ..\n",
    " ..\n",
    " Create \n",
    "\n",
    " # Create Cluster with Woker type with Pools \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
