{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n * Workspace \\n * Add Button > Notebook \\n * Name: \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP1 :CREATE A C;USTER \n",
    "# https://docs.databricks.com/en/getting-started/quick-start.html\n",
    "'''\n",
    "- Compute\n",
    "- Create compute \n",
    "- 12.2 LTS( Scala 2.12 , Spark3.3.2)\n",
    "- Create Cluster\n",
    "'''\n",
    "\n",
    "# Step 2: Create a Notebook \n",
    "'''\n",
    " * Workspace \n",
    " * Add Button > Notebook \n",
    " * Name: \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOption 1: Create a Spark table from the CSV data\\n\\ndrop table if exists diamonds\\nCREATE TABLE diamonds USING CSV OPTIONS (path \"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\", header \"true\")\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 3: Create a table\n",
    "'''\n",
    "Option 1: Create a Spark table from the CSV data\n",
    "\n",
    "drop table if exists diamonds\n",
    "CREATE TABLE diamonds USING CSV OPTIONS (path \"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\", header \"true\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n select \\n   COLOR, avg(price) as price \\n FROM diamonds \\n GROUP BY COLOR \\n ORDER BY COLOR\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query the Table \n",
    "'''\n",
    " select \n",
    "   COLOR, avg(price) as price \n",
    " FROM diamonds \n",
    " GROUP BY COLOR \n",
    " ORDER BY COLOR\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDisplay a chart of the average diamond price by color.\\n\\n1. Next to the Table tab, click + and then click Visualization.\\n   The visualization editor displays.\\n2. In the Visualization Type drop-down, verify that Bar is selected.\\n3. Clear the Horizontal chart checkbox.\\n4. Change the aggregation type for the y columns from Sum to Average.\\n5. Click Save.\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Display the data\n",
    "'''\n",
    "Display a chart of the average diamond price by color.\n",
    "\n",
    "1. Next to the Table tab, click + and then click Visualization.\n",
    "   The visualization editor displays.\n",
    "2. In the Visualization Type drop-down, verify that Bar is selected.\n",
    "3. Clear the Horizontal chart checkbox.\n",
    "4. Change the aggregation type for the y columns from Sum to Average.\n",
    "5. Click Save.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n I should Study \\n  * S3 \\n  * IAM \\n  * GLUE \\n  * EMR \\n  * PYSPARK\\n  * ATHENA\\n  * STEP FUNCTION / \\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regulary \n",
    "'''\n",
    " I should Study \n",
    "  * S3 \n",
    "  * IAM \n",
    "  * GLUE \n",
    "  * EMR \n",
    "  * PYSPARK\n",
    "  * ATHENA\n",
    "  * STEP FUNCTION / \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kk/vsk5n5vj28n42f90qn__wm7w0000gn/T/ipykernel_1613/819706955.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Parallelize the queries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mqueries_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Use map to execute the queries and collect the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Parallelize the queries\n",
    "queries_rdd = sc.parallelize(queries)\n",
    "\n",
    "# Use map to execute the queries and collect the results\n",
    "results = queries_rdd.map(lambda query: \n",
    "                          Row(query=query, result=spark.sql(query).collect()\n",
    "                              )\n",
    "                        )\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = spark.createDataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Row\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame with your queries\n",
    "data = [\n",
    "    (\"SELECT COUNT(*) FROM table1\",),\n",
    "    (\"SELECT COUNT(*) FROM table2\",),\n",
    "    (\"SELECT COUNT(*) FROM table3\",),\n",
    "    # Add more queries\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"query\"])\n",
    "\n",
    "# Function to execute a query\n",
    "def execute_query(query):\n",
    "    result = spark.sql(query).collect()\n",
    "    return Row(query=query, result=result)\n",
    "\n",
    "# Use concurrent.futures.ThreadPoolExecutor to run queries in parallel\n",
    "executor = ThreadPoolExecutor(max_workers=10)  # You can adjust the number of workers as needed\n",
    "\n",
    "# Execute the queries and collect results\n",
    "results = list(executor.map(execute_query, df.select(\"query\").rdd.map(lambda x: x[0])))\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = spark.createDataFrame(results)\n",
    "\n",
    "# Show the results\n",
    "results_df.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
